{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"\n# Welcome to my note\nI took notes to help me remember and understand the topic better.\n\n# Topics\n## Programming Language\n* [[C++|lang.cpp]]\n* [[Java|lang.java]]\n\n## Media\n### [[Video|media.video]]\n* [[h264|media.video.codec.h264]]\n\n## Development\n* [[Elasticsearch|dev.elasticsearch]]\n* [[React|dev.react]]\n* [[NextJs|dev.nextjs]]\n\n## [[Finance tools|finance]]\n* [[Ledger|finance.ledger]]\n* [[Hledger|finance.hledger]]\n\n\n---\n# Welcome to Dendron\n\nThis is the root of your dendron vault. If you decide to publish your entire vault, this will be your landing page. You are free to customize any part of this page except the frontmatter on top.\n\n## Lookup\n\nThis section contains useful links to related resources.\n\n- [Getting Started Guide](https://link.dendron.so/6b25)\n- [Discord](https://link.dendron.so/6b23)\n- [Home Page](https://wiki.dendron.so/)\n- [Github](https://link.dendron.so/6b24)\n- [Developer Docs](https://docs.dendron.so/)\n","n":0.107}}},{"i":2,"$":{"0":{"v":"Templates","n":1}}},{"i":3,"$":{"0":{"v":"Media","n":1}}},{"i":4,"$":{"0":{"v":"Codec","n":1},"1":{"v":"\n## ${title}\n\n## Pros\n\n## Cons\n","n":0.5}}},{"i":5,"$":{"0":{"v":"Proj","n":1}}},{"i":6,"$":{"0":{"v":"TS","n":1}}},{"i":7,"$":{"0":{"v":"Tool","n":1}}},{"i":8,"$":{"0":{"v":"Change Pid","n":0.707},"1":{"v":"\n# Program for changing pid for ts file\nThis is coded in C++.\n\n## changePid flow\n- Find PAT (pid 0 at 12-25 bit)\n- find PMT packet in PAT\n  - if multiple pmt,\n    - find every PMT packets, find if it includes oldPid stream\n      - if yes, set that PMT pid, and modified all that PMT packets with oldPid to newPid\n## changePmtPid flow\n- find PAT -> pid is 0, table id is 0, \n- find all PMT packets by oldPid\n- modified all its pid to newPid\n## Design of the program logic\nPMT packets: (0-base index)\nfind PAT (pid 0 at 12-25 bit) -> \n    check if there is adaption field control (at 26-27 bit) \n    if (01) -> payload starts at (if payload_unit_start_indicator is 1)40 bit + (check pointer_field number of bytes at 36bit)\n    or (if PUSI is 0, start at 36 bit)\n    if (11) -> payload starts at 40bit + (check adaptation_field_length 8 bits number of bytes)\n    if (10 or 00) skip (cuz no payload)\n        PMT pid is at 91-103 bit (assuming only 1 program)\nfind packets with PMT pid (at 11-23 bit)\n    check if there is adaption field control (at 26-27 bit) \n    if (01) -> payload starts at (if payload_unit_start_indicator is 1)40 bit + (check pointer_field number of bytes at 36bit)\n    or (if PUSI is 0, start at 36 bit)\n    if (11) -> payload starts at 40bit + (check adaptation_field_length 8 bits number of bytes)\n    if (10 or 00) skip (cuz no payload)\n        set this payload offset\n        set pidInfo offset -> payloadOffset + (86+95 bit program_info_length) + program_info_length bytes * 8(if any)\n        the pid (to be removed) is at  pidInfo_offset + (11-23bit)\n        find if any ES_info_length at pidInfo_offset + (30-39bit number of bytes)\n        if (found) remove from pidInfo_offset to ES_info bit\n            calculate how many bits are remove -> translate to byte\n        need to change section_length -> from payloadOffset + (12-23 bit) to subtract the bits to be removed\n\n## Design of the usage\n* Change pid\n  * ./program changePid inputfilename --out <> --old oldPid --new newPid\n    * (optional) --from packetno --to packetno\n* Remove pid\n  * ./program removePid inputfilename --out <> pid\n    * (optional) --from packetno --to packetno\n* Change pmt pid\n  * ./program changePmtPid inputfilename --out <> --old oldPid --new newPid\n    * (optional) --from packetno --to packetno\n","n":0.052}}},{"i":9,"$":{"0":{"v":"Media","n":1}}},{"i":10,"$":{"0":{"v":"Video","n":1},"1":{"v":"# Video information\n## Codec\nH.264 MPEG-4\n## Container / File format\nA general rules to hold the data in specific structure, e.g. video stream, audio stream, metadata, etc.\n\n* MPEG-4 part 14 Container (`.mp4`)\n* HLS (HTTP Live Streaming - by Apple) (`.m3u8`)\n  build on Adaptive Bitrate. Deliver over HTTP for wide range of devices across the internet\n* DASH (Dynamic Adaptive Streaming over HTTP) (`.dash`)\n  Open-source and build on Adaptive Bitrate\n\n* watch later\nhttps://www.youtube.com/watch?v=gxefuXizO04\n","n":0.121}}},{"i":11,"$":{"0":{"v":"Codec","n":1},"1":{"v":"\n# Codec\nMeaning coder and decoder. Different codec has different method to encode and decode the original stream.\n\nEncode the original video / audio stream smaller \n","n":0.2}}},{"i":12,"$":{"0":{"v":"Vp9","n":1},"1":{"v":"\n## VP9\n* developed by Google\n* mostly for YouTube\n* royalty-free and open-source\n* good for high resolution and live streaming\n* More consistent than [[HEVC|media.video.codec.h265]]\n\n## Container\n`.webm`\n`.ivf`\n\n### Cons\n* difficult to encode and decode\n* less supported than H.264\n* [[HEVC|media.video.codec.h264]] has better image quality\n","n":0.162}}},{"i":13,"$":{"0":{"v":"H265","n":1},"1":{"v":"# H.265 HEVC (High Efficency Video Coding)\n\n## Container / file format\n.mp4\n\n## Pros\n* higher compress rate than H.264\n* thus half the filesize and bitrate than H.264\n* best for high resolution video source and live streaming\n\n## Cons\n* require more resources to encode\n","n":0.158}}},{"i":14,"$":{"0":{"v":"H264","n":1},"1":{"v":"\n# H.264 AVC (Advanced Video Coding)\nWidely supported video codec\n\n## Container / file format\n.mp4\n.MOV\n\nA codec \n\n## Macroblock\nhttps://course.ece.cmu.edu/~ee899/lecture7.pdf\n\n## Chrominance\n","n":0.243}}},{"i":15,"$":{"0":{"v":"Audio","n":1}}},{"i":16,"$":{"0":{"v":"Codec","n":1},"1":{"v":"# Audio Codec\n\n","n":0.577}}},{"i":17,"$":{"0":{"v":"Mp3","n":1},"1":{"v":"# MP3 MPEG-2 Audio Layer 3\n\n* Lossy codec\n* Encode with human hearing limitation with auditory masking\n* Can compress to 128 kbps\n* Widely supported\n\n## Cons\n* Limited functionality\n","n":0.196}}},{"i":18,"$":{"0":{"v":"Ac3","n":1},"1":{"v":"## AC-3 (Dolby Digital Audio Codec 3)\n* 5.1 surround sound\n* use full range of audio channels\n\n# Cons\n* Not widely supported by devices\n","n":0.213}}},{"i":19,"$":{"0":{"v":"Aac","n":1},"1":{"v":"## AAC (Advanced Audio Coding)\n* More efficient than MP3\n* Same bitrate but better sound quality\n\n## Cons\n* Limit on audio channel\n","n":0.224}}},{"i":20,"$":{"0":{"v":"Lang","n":1}}},{"i":21,"$":{"0":{"v":"Java","n":1},"1":{"v":"\n# Java\n","n":0.707}}},{"i":22,"$":{"0":{"v":"Topic","n":1}}},{"i":23,"$":{"0":{"v":"Call by Value","n":0.577},"1":{"v":"\n# Call by value vs Call by reference\n\nJava is call by value\n\n","n":0.289}}},{"i":24,"$":{"0":{"v":"Cpp","n":1},"1":{"v":"\n# C++\n","n":0.707}}},{"i":25,"$":{"0":{"v":"Key Notation","n":0.707},"1":{"v":"\n|      Notations      |              Key               | Symbol |        Value         |\n| :-----------------: | :----------------------------: | :----: | :------------------: |\n|      `<S-...>`      |           shift-key            |   ⇧    |     shift \\`<S-`     |\n|      `<C-...>`      |   control-key or command key   |   ^    |  control ctrl `<C-`  |\n| `<M-...>`/`<A-...>` |      alt-key or meta-key       |   ⎇    | meta alt `<M-`/`<A-` |\n|      `<D-...>`      |  command-key (Macintosh only)  |   ⌘    |        `<D-`         |\n|      `<t_xx>`       | key with \"xx\" entry in termcap |        |                      |\n","n":0.118}}},{"i":26,"$":{"0":{"v":"Journal","n":1}}},{"i":27,"$":{"0":{"v":"Finance","n":1},"1":{"v":"Plain-Text Accounting\n\n[[Hledger|finance.hledger]]\n[[Ledger|finance.ledger]]\n","n":0.707}}},{"i":28,"$":{"0":{"v":"Ledger","n":1},"1":{"v":"\n# Ledger\nCommand-line Accounting\n\n## Installation\n* git clone git://github.com/ledger/ledger.git\n* cd ledger\n* Install dependencies\n  ```sh\n  sudo apt update\n  sudo apt-get install build-essential cmake doxygen \\\n     libboost-system-dev libboost-dev python3-dev gettext git \\\n     libboost-date-time-dev libboost-filesystem-dev \\\n     libboost-iostreams-dev libboost-python-dev libboost-regex-dev \\\n     libboost-test-dev libedit-dev libgmp3-dev libmpfr-dev texinfo tzdata\n  ```\n* ```sh\n  ./acprep update # Update to the latest, configure, make\n  ```\n* Use with\n  ```sh\n  ./ledger \n  ```\n* Or add to $PATH for global usage\n  ```sh\n  sudo ln -s /<abs_path>/ledger/ledger /usr/local/bin/ledger\n  ```\n\n\n","n":0.117}}},{"i":29,"$":{"0":{"v":"Hledger","n":1},"1":{"v":"\n# Hledger\nBased on Ledger-Cli. Coded with Haskell.\n\n## Installation\n* Download the latest release: https://github.com/simonmichael/hledger/releases (take the major release version)\n* Download the appropriate \"hledger-PLATFORM.zip\" file below.\n* Unzip it to get 2 or 3 hledger binaries in the current directory.\n* On GNU/Linux, you will need to chmod +x these files to make them executable. They should run on most GNU/Linux machines with x64 or (when provided) arm32v7 architecture.\n* Use with\n  ```sh\n  ./hledger-linux-static-x64 -f .hledger.journal print\n  ```\n* Or add to $PATH for global usage\n  ```sh\n  sudo ln -s /<abs_path>/hledger-linux-static-x64 /usr/local/bin/hledger\n  ```\n\n## Translate csv to journal\n### File format\n1. A journal file\n   ```\n   ; $HOME/.hledger.journal\n   2020-01-01 opening balances\n       assets:checking         $1234\n       equity\n   \n   2020-03-15 client payment\n       assets:checking         $2000\n       income:consulting\n   \n   2020-03-20 Sprouts\n       expenses:food:groceries  $100\n       assets:cash               $40\n       assets:checking\n   ```\n\n2. My csv file exported from Money Manager\n\n   | Date  | Account | Category | SubCategory | Note  |  HKD  | Income/Expense | Note  | Amount | Currency | Account |\n   | :---: | :-----: | :------: | :---------: | :---: | :---: | :------------: | :---: | :----: | :------: | :-----: |\n   |       |         |          |             |       |       |                |       |        |          |         |\n   \n   The concerned columns are:\n   \n   | Date  |    Account    |        Category         |     SubCategory     |    Note     | ~~HKD~~ |            Income/Expense             | Note (additional note) | Amount | Currency | ~~Account~~ |\n   | :---: | :-----------: | :---------------------: | :-----------------: | :---------: | :-----: | :-----------------------------------: | :--------------------: | :----: | :------: | :---------: |\n   | date  | asset account | expense / asset account | expense subcategory | description |   ---   | condition (income, expense, transfer) |    additional note     | amount | currency |     ---     |\n\n### Conversion logic\n* To convert from csv to journal\n  1. Specify all assets with groups\n     1. e.g. assets:bank:HSBC.\n     2. assets:credit card:SimplyCash\n     3. assets:digital wallet\n  2. For all asset account -> need to match account with the specified assets\n  3. If 'Income/Expense' column is `Income`\n     1. asset account1 -> account\n     2. account2 -> income:<Category\\>:<SubCategory\\>\n     3. amount1 -> **+**amount\n     4. amount2 -> -amount\n  4. else if 'Income/Expense' column is `Expense`\n     1. asset account1 -> account\n     2. account2 -> expense:<Category\\>:<SubCategory\\>\n     3. amount1 -> **-**amount\n     4. amount2 -> +amount\n  5. else if 'Income/Expense' column is `Trasnsfer-Out`\n     1. asset account1 -> account\n     2. account2 -> asset account -> <Category\\>\n     3. amount1 -> **-**amount\n     4. amount2 -> +amount\n  6. currency -> currency\n  7. Date -> date\n  8. Note -> note\n  9. additional note -> additional note\n\njournal format: https://hledger.org/add.html\n","n":0.05}}},{"i":30,"$":{"0":{"v":"Dev","n":1}}},{"i":31,"$":{"0":{"v":"React","n":1},"1":{"v":"\nReact is a JavaScript Library for building user interfaces.\nhttps://reactjs.org/tutorial/tutorial.html\nhttps://codepen.io/gaearon/pen/oWWQNa?editors=0011\ntic-tac-toe game\n```js\n/**\nclass Square extends React.Component {  // a controlled components. will not do any action\n  render() {\n    return (\n      <button \n        className=\"square\"\n        onClick={() => {\n          this.props.onClick(); // this send the function from parent (Board) to Square. props is passed as Component's parameter\n          // this.setState({value: 'X'});  // this set the state object\n          console.log('click-' + this.props.value);\n        }}\n      >\n        {this.props.value}\n      </button> // this value is what will be shown on the board\n    );\n  }\n}\n*/\n\nfunction Square(props) {  // a function component does not extend React.Component. Only need the input props param\n  return (\n    <button\n      className=\"square\"\n      onClick={props.onClick} // both () are removed\n    >\n      {props.value}\n    </button> // no longer have this\n  );  \n}\n\nclass Board extends React.Component {\n  constructor(props) { // make a state to store the value of this component\n    super(props);\n    this.state = {  // a component has props, and now we add 'state' field. We put 'value' to 'state' object\n      squares: Array(9).fill(null),\n      xIsNext: true,\n    }; // the state is stored in Board instead of individual Square components\n  }\n  \n  handleClick(i) {\n    const squares = this.state.squares.slice(); // make a shallow copy of squares array to modify instead of the existing array\n    if (calculateWinner(squares) || squares[i]) { // ignore click after winner or board is filled\n      return;\n    }\n    squares[i] = this.state.xIsNext? 'X' : 'O'; // on that index, it changes from null to 'X'\n    this.setState({\n      squares: squares,\n      xIsNext: !this.state.xIsNext,\n    });  // update the state of the board. And will re-render Square's components\n  }\n\n  renderSquare(i) {\n    return (\n      <Square\n        value={this.state.squares[i]} // = Square.props.value\n        onClick={() => this.handleClick(i)}\n      />\n    );\n  }\n\n  render() {\n    const winner = calculateWinner(this.state.squares);\n    let status;\n    if (winner) { // if there is winner\n      status = \"Winner is \" + winner;\n    } else {\n      status = 'Next player: ' + (this.state.xIsNext ? 'X' : 'O');\n    }\n\n    return (\n      <div>\n        <div className=\"status\">{status}</div>\n        <div className=\"board-row\">\n          {this.renderSquare(0)}\n          {this.renderSquare(1)}\n          {this.renderSquare(2)}\n        </div>\n        <div className=\"board-row\">\n          {this.renderSquare(3)}\n          {this.renderSquare(4)}\n          {this.renderSquare(5)}\n        </div>\n        <div className=\"board-row\">\n          {this.renderSquare(6)}\n          {this.renderSquare(7)}\n          {this.renderSquare(8)}\n        </div>\n      </div>\n    );\n  }\n}\n\nclass Game extends React.Component {\n  render() {\n    return (\n      <div className=\"game\">\n        <div className=\"game-board\">\n          <Board />\n        </div>\n        <div className=\"game-info\">\n          <div>{/* status */}</div>\n          <ol>{/* TODO */}</ol>\n        </div>\n      </div>\n    );\n  }\n}\n\n// ========================================\n\nReactDOM.render(\n  <Game />,\n  document.getElementById('root')\n);\n\nfunction calculateWinner(squares) {\n  const lines = [\n    [0, 1, 2],\n    [3, 4, 5],\n    [6, 7, 8],\n    [0, 3, 6],\n    [1, 4, 7],\n    [2, 5, 8],\n    [0, 4, 8],\n    [2, 4, 6],\n  ];\n  for (let i = 0; i < lines.length; i++) {\n    const [a, b, c] = lines[i];\n    if (squares[a] && squares[a] === squares[b] && squares[a] === squares[c]) {\n      return squares[a];\n    }\n  }\n  return null;\n}\n```\n","n":0.049}}},{"i":32,"$":{"0":{"v":"Nextjs","n":1}}},{"i":33,"$":{"0":{"v":"Elasticsearch","n":1},"1":{"v":"\nReference: https://harmonic-owlstars.udemy.com/course/elasticsearch-complete-guide/\nGithub for queries example: https://github.com/codingexplained/complete-guide-to-elasticsearch\n\n## Installation\nElasticsearch comes with a bunch of jar files, along with Apache Lucene library.\nThere is no need for installation but download the elasticsearch jar file.\n\nDownload the zip / tar file\nthen run\n```sh\nbin\\elasticsearch.bat #or\nbin/elasticsearch\n```\n!!! Keep the record of password, http cert and enrollment token shown on the console log\nThis is needed for access/requesting elasticsearch with security enabled\n\n\nTo reset password for default user 'elastic', need to run the elasticsearch first,\nand run the below command in another terminal\n```sh\nbin\\elasticsearch-reset-password.bat -u elastic --url \"https://127.0.0.1:9200\"\n```\n\nTo access elasticsearch with security enabled\n```sh\ncurl --cacert config/certs/http_ca.crt -u elastic https://localhost:9200\n# with password on the line\ncurl --cacert config/certs/http_ca.crt -u elastic:password https://localhost:9200\n```\nReference: https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html\n\n\n## Config\nChange any configurations in `elastic.yml` inside `config/` folder.\ne.g. Ip address, port\n\n## Architecture of Elasticsearch\nWhen we start up Elasticsearch(ES), we start up a node.\n* **Node** - an instance of ES that stores the data.</br>\n  Bring up more node to store more data, each node stores part of the data.</br>\n  Multiple nodes can be run on a same machine.</br>\n  It always resides in a cluster.\n* **Cluster** - a collection of nodes that together contains all of the our data.</br>\n  Usually one cluster is enough without the need of cross-cluster searches.</br>\n  Still, you can run different clusters for different purposes. This can separate things logically and config differently.\n* **document** - A unit of data that stored in the cluster. A JSON object with metadata along with your stored desire data.</br>\n   ```json\n   {\n       \"name\": \"Mark\",\n       \"country\": \"England\"\n   }\n   ```\n   is stored in `document` like this inside `_source`\n   ```json\n   {\n       \"_index\": \"people\",\n       \"_type\": \"doc\",\n       \"_id\": \"123d2f\",\n       \"_version\": 1,\n       \"_seq_no\": 0,\n       \"_primary_term\": 1,\n       \"_source\": {\n           \"name\": \"Mark\",\n           \"country\": \"England\"\n       }\n   }\n   ```\n   *Documents* are grouped together with *index* or *indices*\n### Sharding\n[**Scalibility**] - Divide indices into small piece, called a *shard*. Thus, sharding is done on index level. It can scale horizontally with data volume.\n\n[**Distributed data**] - Data in one index can split into shards and place onto different nodes. It is not neccessarily to be placed on separate different nodes, some shards can place onto the same node while others are in another nodes.\n\nEach shard is independent.\n[**Parallelized search**] - Sharding improves performance by running searches on separate shards at the same time. With this, it can also utilize the hardware of separate nodes.\n\nOnce the index is created, the shard number cannot be changed.\nWith ES >= 7.0.0, default shard number is 1. **Split API** can be used to increase the shard number after creation. It involves creating new index. In contrast, **Shrink API** can be used to decrease the shard number.\n\n### Replication\n[**Redundancy**] - Having a full replication of data can prevent a single failure on hardward causing data lost. It copies shards as *replica shards*. *Primary shard* means the shard is replicated. *Replication group* means the replication of shards and original shard.\nReplica shards are NEVER stored in the same node of primary shard.\n\n[**Query Performance**] - Having replica shards can improve search performance. CPU parallelization is used if multiple shards on the same nodes. E.g. if 3 requests incoming, 3 requests can be run on 3 different replica shards at the same time. CPU can run simultaneous task with threads and query different shards on the same node.\n\n### Snapshot\n[**Backup**] - Having snapshots can do backup before doing some (critical/risky) changes to the database.\n\n### Node\n* Master - creating, deleting indices, keeping track of nodes\n* Data - store the data shards, handle search query to data\n* Ingest - ingest pipeline for processing data before adding to index\n* Reference: https://www.elastic.co/guide/en/elasticsearch/reference/8.0/modules-node.html\n\n\n","n":0.042}}},{"i":34,"$":{"0":{"v":"Usage","n":1},"1":{"v":"\n## Query\n(This is used in Kibana Dev Tools)\n```\nGET /_cluster/health\n    --------- ------\n       API    command\nGET /_cat/nodes?v\n    ----  ----  --\n                query parameter\n```\n\nAPI - begins with `_` by convention\n\n### List of APIs\n|  api   | meaning                                                                  |\n| :----: | ------------------------------------------------------------------------ |\n| `_cat` | output data into human-readable format.</br> command: `indices`, `nodes` |\n\n### List of query parameters\n| query param | meaning                                             |\n| :---------: | --------------------------------------------------- |\n|    `?v`     | verbose - show the descriptive header in the output |\n\n## cURL\nCopy from Kibana Dev Tool command\n```sh\ncurl -XGET \"https://localhost:9200/.kibana/_search\" -H 'Content-Type: application/json' --cacert /e/elasticsearch/elasticsearch-8.0.1-windows-x86_64/elasticsearch-8.0.1/config/certs/http_ca.crt -u elastic:ww=86Lhxttvw8D_zEqr1 -d'\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}'\n```\n* The `-X GET` to specify HTTP action/verb.\n* `-H 'Content-Type: application/json'` Content-type header is required when having request body (which is in JSON format). Otherwise Elasticsearch will reject the request.\n* YAML or other formats are supported as well.\n\n\n## Add index\nE.g. add `pages` index to the database\n```\nPUT /pages\n```\n* `GET /_cluster/health` and `GET /_cat/indices'v` shows the newly added `pages` index has <span style=\"color:yellow\">*yellow*</span> health. Because the replica shard is **unassigned** as there is only 1 node.\n  ```\n  health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n  yellow open   pages eyrfJpxxQIWBz7q5sucgrw   1   1          0            0       225b           225b\n  ```\n\nTo add index with specify shard and replica\n```\nPUT /products\n{\n  \"settings\": {\n    \"number_of_shards\": 2,\n    \"number_of_replicas\": 2\n  }\n}\n```\n\n## Adding Node\n1. Download another set of ES files in another directory\n2. In the original ES directory, obtain the enrollment token\n   ```sh\n   bin\\elasticsearch-create-enrollment-token.bat -s node\n   ```\n3. Modify the config/elasticsearch.yml for `cluster.name` and `node.name`. If it is running on another different machine, more config have to be changed\n4. Start up ES with security enabled by passing the enrollment token obtained previously\n   ```sh\n   bin\\elasticsearch.bat --enrollment-token <enrollment-token>\n   ```\n\n## Adding documents (data)\n`POST, /<index>/_doc` followed by the json object for the data.\n(the `_doc`) means this is a document\n\n```\nPOST /products/_doc\n{\n  \"name\": \"Coffee\",\n  \"price\": 34,\n  \"in_stock\": 10\n}\n```\nresult:\n```\n{\n  \"_index\" : \"products\",\n  \"_id\" : \"42j-T38B_ZjsYwbHxq8s\",\n  \"_version\" : 1,\n  \"result\" : \"created\",\n  \"_shards\" : {\n    \"total\" : 3,\n    \"successful\" : 2,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1\n}\n```\n`_shards` shows the status of the data stored in shards. -> `total`: primary + number of replica shards.\n\nTo specify `_id` of the document -> `PUT /<index>/_doc/<id>`\n```\nPUT /products/_doc/23iD20x\n{\n  xxx (the json object)\n}\n```\n\n## Getting document\n```\nGET /<index>/_doc/<id>\nGET /products/_doc/100\n```\nThe result shows the json object inside `_source`\n```\n{\n  \"_index\" : \"products\",\n  \"_id\" : \"100\",\n  \"_version\" : 2,\n  \"_seq_no\" : 2,\n  \"_primary_term\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"Tissue\",\n    \"price\" : 3,\n    \"in_stock\" : 4\n  }\n}\n```\n\n## Update document\n```\nPOST /<index>/_update/<id>\n```\n```\nPOST /products/_update/100\n{\n  \"doc\": {\n    \"in_stock\": 3\n  }\n}\n```\n\nUpdate API -> `/<index>/_update`\nThe document is immutable. The `Update` API is just replacing the existing document with modified document. It saves us effort for sending 2 requests `GET` and `PUT` which might incur network latency and overhead\n\n## Scripted Update\nWithout needing to know the existing data to update the data with script\n```\nPOST /<index>/_update/<id>\n// script key, source key\n// ctx = context\n// _source = access _source field in the document\n// .<field> = the field inside _source\n// --- single line script ---\nPOST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  }\n}\n```\nUse the script with function parameters\n```\nPOST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock -= params.quantity\",\n    \"params\": {\n      \"quantity\": 4\n    }\n  }\n}\n// --- multi-line script ---\nPOST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"\"\"\n      if (ctx._source.in_stock <= 0) {\n        ctx.op = 'noop';\n      }\n      ctx._source.in_stock--;\n    \"\"\",\n    \"params\": {\n      \"quantity\": 4\n    }\n  }\n}\n```\nthe 'noop' = no-operation, means do nothing. If it does nothing, it will not show successful\n`\"_shards\": {\n  \"successful\": 0\n}`\n\n## `Upsert`\nUpdate or insert. If document exists, it updates, otherwise, it inserts.\nIt must have \"`script`\" or \"`doc`\" for update\n```\nPOST /products/_update/101\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock++\"\n  },\n  \"upsert\": {\n    \"name\": \"tv\",\n    \"price\": 2444,\n    \"in_stock\": 5\n  }\n}\n```\nCan check the response \"`result`\" is `created` or `updated`\n```\n{\n  \"_index\" : \"products\",\n  \"_id\" : \"101\",\n  \"_version\" : 3,\n  \"result\" : \"updated\",\n  \"_shards\" : {\n    \"total\" : 3,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 26,\n  \"_primary_term\" : 2\n}\n```\n\n## Replace documents\nIt is actually the same as [[Adding document|dev.elasticsearch.usage#adding-documents-data]]\n\nBecause the elasticsearch document is immutable, basically it is just removing and adding new document again.\n\n## Delete documents\n```\nDELETE /<index>/_doc/<id>\nDELETE /products/_doc/101\n``` \n\n## Update by query (update data with matching data)\nwith `script` for update operation and `query` for matching data operation\n```\nPOST /products/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  },\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n```\n\n## Delete by query (delete data with matching data)\ndelete data if it matches the query. Note it is using `POST`\n```\nPOST /products/_delete_by_query\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n```\n\n## Batch Processing\nBulk API request. The header content-type has to be `Content-Type: application/x-ndjson`.\nThere are 4 actions: index (add or overwrite the document), create (add document if not exist), update, delete\nThe document has to be in one line for document or query.\nAnd it has to be line separated!! And for the last line, it requires a empty new line\nIt is good for having script to generate the request for large amount of data manipulation\n```\nPOST /_bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": 200 }}\n{ \"name\": \"Reader\", \"price\": 19, \"in_stock\": 39 }\n{ \"create\": { \"_index\": \"products\", \"_id\": 201 }}\n{ \"name\": \"Bread\", \"price\": 2, \"in_stock\": 100 }\n\n// can use index in the api, thus no need to put the index in every line\nPOST /products/_bulk\n{ \"update\": { \"_id\": 201 }}\n{ \"doc\": { \"price\": 22}}\n{ \"delete\": { \"_id\": 200}}\n```\n\n### Using cURL\nMake sure the Header content type is using `x-ndjson` and the file is read by `--data-binary` to preserve the last empty new line in the file.\n```\ncurl -XPOST -H 'Content-Type: application/x-ndjson' --cacert /e/elasticsearch/elasticsearch-8.0.1-windows-x86_64/elasticsearch-8.0.1/config/certs/http_ca.crt -u elastic:password https://localhost:9200/products/_bulk --data-binary \"@products-bulk.json\"\n```","n":0.034}}},{"i":35,"$":{"0":{"v":"Kibana","n":1},"1":{"v":"\n## Installation\nSimilar to [[Elasticsearch installation|dev.elasticsearch#installation]]\nOnly need to run the command to bring up the Kibana\n```sh\nbin\\kibana.bat #or\nbin/kibana\n```\nIt requires the Elasticsearch to be running first.\n\nFor security enabled login\n1. Then open in the browser with [http://localhost:5601](http://localhost:5601)\n   Or click the link shown on the console log\n2. Paste the enrollment token that you copied and click the button to connect your Kibana instance with Elasticsearch. </br>\n   If the token expires (only last for 30mins), generate new token with command:\n   ```sh\n   bin\\elasticsearch-create-enrollment-token.bat -s kibana\n   ```\n3. Log in to Kibana as the elastic user with the password that was generated when you started Elasticsearch</br>\n   ```\n   user: elastic\n   pw: xxxxxxxxxxxx\n   ```\n\n## Config\nChange any configurations in `kibana.yml` inside `config/` folder.\ne.g. Ip address, port\n\n\n","n":0.094}}},{"i":36,"$":{"0":{"v":"Infomation","n":1},"1":{"v":"\n## Routing\nES uses a routing formula to find the shard location of a document with its ID.\nThe formula includes the hash of ID, and number of shards.\n\ne.g. `_shard_id = hash(_routing) % num_of_shards`\nIf number of shards changes after the index is created, the data might not be found afterwards with incorrect routing result\n\n## Elasticsearch read/write data\nRead - ES has coordinating node that takes the GET request and passes to the best performance shard copy (could be primary or replica shards). This is done by **Adaptive Replica Selection**. The shard copy will return the result to that node, and then is passed to the ES client (e.g. Kibana, or other applications)\n\nWrite - ES's coordinating node taks the request and find the **primary shard**. The shard has to validate the request. Then write to itself locally. Forward the request to its replica shards in parallel.\n\nVersioning - `_version` metadata is placed along with each document. When the document with certain ID is deleted, the ID is saved for 60s. After that, it will reset the version back to '1'. For every update or changes, it will increment `_version` by 1.\nIt could be used for checking the times of update. But this is not that useful currently as the original objective \"optimistic concurrency control\" has now improved with other approaches.\n\nConcurrency Control - when 2 requests are placed, the result might be incorrect. \"`_primary_term`\" and \"`_seq_no`\" are used.\n`_primary_term` is a counter for times that the primary shard is changed. (could be due to hardward failure). This counter is persisted in cluster state.\n`_seq_no` is a counter of number of write operation. Primary shard handles this sequence number.","n":0.061}}},{"i":37,"$":{"0":{"v":"Elasticstack","n":1},"1":{"v":"# Introduction\n* A database that holds data and allow advanced efficient searching.\n* An analytic and search engine.\n* Allows different search methods, like text, search relevance, sorting result.\n* Query and analyze structured data.\n* It scales easily with data volumes and query throughput.\n\nTable is known as ***Index***\nData is stored as ***Documents*** (= row in SQL)\nDocument data in ***Fields*** (= columns in SQL)\n\n| Terminology                                          |\n| ---------------------------------------------------- |\n| Elasticsearch (ES)                                   |\n| ELK = Elasticsearch, Logstash, Kibana                |\n| Elastic Stack = Superset of ELK, which also has Beat |\n\n```mermaid\n  flowchart LR;\n    Logstash & Beat -->ES;\n    ES<-->Kibana;\n    X-pack\n```\n\n## Kibana\n* UI dashboard to visualize the elasticsearch data/result\n* Analyze the data with different visualization\n* It can request data from Elasticsearch\n\n## Logstash\n* An event processing pipeling\n* Process logs from applications, send to Elasticsearch\n* Input (different types of data, log files, computer stats file, json data) ->\n* Filter (how to process different types of data as an event, how to read json, xml or other formats) ->\n* Output (where to stash the data, Elasticsearch, Kafka queue, HTTP endpoints, etc)\n* e.g. a line of log can be translate to an event formatted to json by specific processor\n\n## X-Pack\nAdditional features for Elasticsearch and Kibana\n* Security (authentication / authorization, control user permission)\n* Monitoring (performance of Elastic Stacks, CPU usage)\n* Alerting (CPU usage, disk space, customer request. by e-mail, Slack, other applications)\n* Reporting (export Kibana visualization data, schedule report generation)\n* Machine Learning (abnormality detection, forecasting)\n* Graph (analyze relationship of data, suggest relevance data)\n* Elasticsearch SQL (can use SQL instead of use Query DSL (json))\n\n## Beats\n* Data shippers - send data to Logstash/ES\n* Filebeat - collect logs and send to Logstash/ES\n* Metricbeat - collect system and service statistic\n","n":0.061}}},{"i":38,"$":{"0":{"v":"Dendron","n":1},"1":{"v":"\n## Dendron Usage\n\n```\n.\n└── workspace\n    ├── vault.main\n    │   ├── foo.md\n    │   ├── foo.one.md\n    │   └── foo.two.md\n    └── vault.secret (hypothetical)\n        ├── secret.one.md\n        └── secret.two.md\n```\n\n\n## Workspace\nWorkspace level means the directory that holds `dendron.yml` and/or `dendron.code-workspace`\n\n## Vault\nVault level is the directory that holds `root.md` and possibly holds every other md files\n* to add vault -> `C-S-P`[^keynotation] Dendron:Vault Add\n## Naming Convention\nMarkdown note file naming convention: kebab case\n* `dendron.docs.mash-potato.md -> title: Mash Potato`\n\n[^keynotation]: [[key notation|key.notation]]\n\n\n## Schema\nIt is like a rule for folder structure. It describes your notes hierachy in a parent-children relations\nThe rule can be used for auto-completion when creating/finding new notes for the specific file path\ne.g. `recipe.vege.potato.mash-potato.md`\nit is under the schema of:\n* `recipe.`\n* `recipe.vege.`\n* `recipe.vege.potato.`\n* `recipe.vege.potato.mash-potato.`\nWith Schema, you can add templates with matching schema\nAnd in xxx.schema.yml file, you must define every group. Cannot list the children id without defining its entry.\n`namespace: true` means it has arbitrary children. Meaning it has another middle '*' level before defined `children` level.\n```yml\nversion: 1\nimports: [] # can import other schema file (with filename)\nschemas:\n  - id: media # the name that has to match with the file name\n    title: media # a title you give to this schema (can be diff from id)\n    parent: root # parent of this file. at least one has to set `root`\n    children: # children of this file\n      - video\n      - audio\n  - id: video # you can separate each group by each item\n    title: video\n    children:\n      - codec\n  - id: audio\n    title: audio\n    children:\n      - codec\n  - id: codec\n    namespace: true # this matches the immediate level of children -> codec.xxx, (but not codec.xxx.abc)\n    template:\n      id: templates.media.codec\n      type: note\n```\n\n","n":0.062}}},{"i":39,"$":{"0":{"v":"Setup","n":1},"1":{"v":"\n## Setup dendron\n\n## about dendron-cli\nreference: https://wiki.dendron.so/notes/23a1b942-99af-45c8-8116-4f4bb7dccd21/\n## Upgrade dendron-cli\n```sh\nnpm install --save @dendronhq/dendron-cli@latest\n```\n## Upgrade to latest version of publishing\n```sh\ncd {workspace_dir}\ncd .next\necho \"reset workspace...\"\ngit reset --hard\necho \"pulling latest version...\"\ngit pull\necho \"checking for new dependencies...\"\nnpm install\n``","n":0.177}}},{"i":40,"$":{"0":{"v":"Git","n":1},"1":{"v":"# Create Git page\nReference page: [GitHub Pages with GitHub Actions](https://wiki.dendron.so/notes/FnK2ws6w1uaS1YzBUY3BR/)\n\n### Setup Git\n1. Create the Git repo\n2. Locally in VSCode, initialize a workspace\n3. On workspace level (where dendron.yml locates)\n   ```sh\n   git init\n   git add .\n   git commit -m\"Initial commit\"\n   git branch -M main\n   git remote add origin <the github repo address>\n   git push -u origin main\n   ```\n\n### Setup dendron (for building the webpage with next.js)\n1. On workspace level, initialize dendron-cli\n   ```sh\n   npm init -y \n   npm install @dendronhq/dendron-cli@latest\n   echo .next >> .gitignore\n   npx dendron publish init # init the publish\n   npx dendron publish dev # for checking if the publish works with the md note, browse http://localhost:3000/\n   ```\n2. Configure your `dendron.yml` - Ctrl+Shift+P \\<Dendron: Configure (yaml)>\n   ```yaml\n   site:\n    siteUrl: https://<username>.github.io\n    assetsPrefix: /<repo-name>\n   ```\n    Update like this\n   ```yaml\n   site:\n    siteUrl: https://kevinslin.github.io\n    assetsPrefix: /dendron-publish-sample\n   ```\n\n## Setup Github Page\n1. Create a branch for dendron publishing.\n   ```sh\n   git checkout -b pages\n   git push -u origin HEAD\n   ```\n2. Go to Github Setting to setup the Pages. Select `pages` branch, and select `/root` folder\n3. After building the page by `dendron-cli` (publish export), the html and style files will be stored in this `pages` branch. And Github Page will be built from those in `pages` branch\n\n## (deprecated) Setup the `docs` folder\n**!This is no longer needed as dendron helps the creation of docs folder with the latest updated version.**\n<details>\n<summary>(expand to see) Guide to manually pull the <code>docs/</code> folder</summary>\n\n1. `docs` folder is needed in the git repo for dendron-cli export done in the github action.\n2. Copy the `docs` folder from [template.publish.github](https://github.com/dendronhq/template.publish.github/tree/main/docs)\n   ```sh\n   git remote add -f temp https://github.com/dendronhq/template.publish.github.git\n   git sparse-checkout init # enable sparse-checkout\n   git sparse-checkout set docs/   # only pull `docs`\n   git sparse-checkout list\n   git pull temp main # pull the temp remote repo main branch\n   git status # check if `docs` is added\n   git remote remove temp # remove the temp remote repo\n   git sparse-checkout set # remove the target\n   git sparse-checkout disable # disable sparse-checkout\n   ```\n</details>\n\n## Setup Github Action\n1. Go to `main` branch, at workspace level\n   ```sh\n   git checkout main\n   mkdir -p .github/workflows\n   touch .github/workflows/publish.yml\n   ```\n2. [publish.yml](https://wiki.dendron.so/notes/FnK2ws6w1uaS1YzBUY3BR/#steps---setup-github-actions)\n3. [`action/cache@v2`][action/cachev2] cache the `node-module/` from previous build. And update it if it changes\n4. This also cache `.next` folder from previous build. The next build if found cache, it will restore from it. In this case, it saves time from installing dependencies, initializing next app again\n4. check https://github.com/marketplace/actions/cache\n5. Commit and push the change\n   ```sh\n   git add .\n   git commit -m \"add workflow\"\n   git push\n   ```\n[action/cachev2]: https://github.com/marketplace/actions/cache\n","n":0.05}}},{"i":41,"$":{"0":{"v":"Develop","n":1}}},{"i":42,"$":{"0":{"v":"Nextjs Template","n":0.707},"1":{"v":"\n# Publish page\n```\nnpx dendron publish init # init the publish\n```\n\nThis will pull the [`nextjs-template`][nextjs-template] github repo to `<workspace>/.next` folder\n\n[publish command file entry point][publishCLICommand] \nFirst, it will init the git repo\n* initialize the `nextPath` (meaning the root path of this dendron project) if not initialized\n* clone the [nextjs-template] with [NextjsExportPod] to root path of `.next` folder\n* then install dependencies -> `npm install`\n\nIf initialized,\n* it will update the [nextjs-template] repo instead,\n* pull the latest commit of the main branch.\n* then install dependencies -> `npm install`\n\n\n[publishCLICommand]: https://github.com/dendronhq/dendron/blob/master/packages/dendron-cli/src/commands/publishCLICommand.ts\n[NextjsExportPod]: https://github.com/dendronhq/dendron/blob/cfffd6a1988c372f7472bb2cd93126befd866a0d/packages/pods-core/src/builtin/NextjsExportPod.ts\n[nextjs-template]: https://github.com/dendronhq/nextjs-template\n\n```\nnpx dendron publish dev\n```\nThe above will call `next dev` in [nextjs-template] repo. Test your built web page in development\n\n```\nnpx dendron publish export\n```\nThe above will call `next export` (meaning `next build && next export`) in [nextjs-template] repo.\n\n```\nnpx dendron publish export --target github --yes\n```\nWith `--target github` flag, this will create `docs/` folder and move the built app - `.next/out/` folder to `docs/` folder.\n`--yes` is used to automatically remove `docs/` folder if existed.\n\n\n### Found bug on github publish\n-> https://github.com/dendronhq/nextjs-template/blob/main/pages/_app.tsx\nchange the \n```md\n<link rel=\"icon\" href=\"/favicon.ico\" />\n```\nto (remove the '/' as Github takes the same level as `index.html` instead of root level (which is '/' of `/home/worker/<repo-name>/<repo-name>/<publish_dir>`))\n```md\n<link rel=\"icon\" href=\"favicon.ico\" />\n```\nReference: [(CI/CD): Deploy Next.js (SSG) to GitHub Pages using GitHub Actions](https://youtu.be/yRz8D_oJMWQ?t=1068)\n","n":0.071}}}]}
